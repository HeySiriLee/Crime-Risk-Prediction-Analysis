{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e597ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ì„œìš¸ì‹œ ë²”ì£„ ì˜ˆì¸¡ ëª¨ë¸ - Feature Engineering & Machine Learning\n",
    "# Step 3: í•™ìŠµ ë°ì´í„°ì…‹ êµ¬ì¶• ë° ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í•™ìŠµ\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def set_korean_font():\n",
    "    \"\"\"ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ ìë™ ê°ì§€ ë° ì„¤ì •\"\"\"\n",
    "    \n",
    "    # í°íŠ¸ ìš°ì„ ìˆœìœ„ ë¦¬ìŠ¤íŠ¸ (macOS â†’ Linux â†’ Windows)\n",
    "    font_candidates = [\n",
    "        'AppleGothic',           # macOS ê¸°ë³¸\n",
    "        'Apple SD Gothic Neo',   # macOS\n",
    "        'NanumGothic',          # ë‚˜ëˆ”ê³ ë”•\n",
    "        'NanumBarunGothic',     # ë‚˜ëˆ”ë°”ë¥¸ê³ ë”•\n",
    "        'Malgun Gothic',        # ë§‘ì€ ê³ ë”• (Windows)\n",
    "        'Noto Sans CJK KR',     # êµ¬ê¸€ Noto\n",
    "        'DejaVu Sans'           # í´ë°± (í•œê¸€ ì•ˆë˜ì§€ë§Œ ê¹¨ì§€ì§€ëŠ” ì•ŠìŒ)\n",
    "    ]\n",
    "    \n",
    "    # ì‹œìŠ¤í…œì— ì„¤ì¹˜ëœ í°íŠ¸ ëª©ë¡\n",
    "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "    \n",
    "    # ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ í°íŠ¸ ì°¾ê¸°\n",
    "    selected_font = None\n",
    "    for font in font_candidates:\n",
    "        if font in available_fonts:\n",
    "            selected_font = font\n",
    "            break\n",
    "    \n",
    "    # í°íŠ¸ ì„¤ì •\n",
    "    if selected_font:\n",
    "        plt.rc('font', family=selected_font)\n",
    "        print(f\"âœ… í•œê¸€ í°íŠ¸ ì„¤ì •: {selected_font}\")\n",
    "    else:\n",
    "        # í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° - ìœ ë‹ˆì½”ë“œ ì‚¬ìš©\n",
    "        plt.rcParams['axes.unicode_minus'] = False\n",
    "        print(\"í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        print(\"í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‚˜ëˆ”ê³ ë”• ì„¤ì¹˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    \n",
    "    return selected_font\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "selected_font = set_korean_font()\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸš€ ì„œìš¸ì‹œ ë²”ì£„ ì˜ˆì¸¡ ëª¨ë¸ - Feature Engineering & ML í•™ìŠµ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ë°ì´í„° ìƒì„± (ì´ì „ ë‹¨ê³„ ë°ì´í„° ê¸°ë°˜ ìƒ˜í”Œ)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š [Step 1] ê¸°ì´ˆ ë°ì´í„° ìƒì„±\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# 1-1. ì‹œê°„ëŒ€ ë¶„í¬ ë°ì´í„° (df_time_final)\n",
    "time_data = {\n",
    "    'ë²”ì£„ë¶„ë¥˜': ['ì‚´ì¸', 'ê°•ë„', 'ì„±í­ë ¥', 'ì ˆë„', 'í­ë ¥'],\n",
    "    '00ì‹œ-03ì‹œ': [89, 127, 5765, 15234, 25678],\n",
    "    '03ì‹œ-06ì‹œ': [67, 98, 3456, 8765, 12345],\n",
    "    '06ì‹œ-09ì‹œ': [45, 65, 2345, 12456, 18765],\n",
    "    '09ì‹œ-12ì‹œ': [78, 89, 4567, 25678, 34567],\n",
    "    '12ì‹œ-15ì‹œ': [92, 112, 5678, 32456, 45678],\n",
    "    '15ì‹œ-18ì‹œ': [98, 134, 6234, 38765, 52345],\n",
    "    '18ì‹œ-21ì‹œ': [112, 156, 7890, 42345, 58765],\n",
    "    '21ì‹œ-24ì‹œ': [95, 143, 6543, 28765, 42345]\n",
    "}\n",
    "df_time_final = pd.DataFrame(time_data)\n",
    "\n",
    "# 1-2. ì¥ì†Œ ë¶„í¬ ë°ì´í„° (df_place_final)\n",
    "place_data = {\n",
    "    'ë²”ì£„ë¶„ë¥˜': ['ì‚´ì¸', 'ê°•ë„', 'ì„±í­ë ¥', 'ì ˆë„', 'í­ë ¥'],\n",
    "    'ì£¼ê±°ì§€êµ¬': [312, 234, 18765, 45678, 98765],\n",
    "    'ì—…ë¬´ì§€êµ¬': [89, 145, 8765, 32456, 54321],\n",
    "    'ë‹¤ì¤‘ì´ìš©ì‹œì„¤': [156, 387, 15432, 98765, 156789],\n",
    "    'ê¸°íƒ€': [119, 158, 5516, 27565, 45555]\n",
    "}\n",
    "df_place_final = pd.DataFrame(place_data)\n",
    "\n",
    "# 1-3. ìì¹˜êµ¬ë³„ ì—°ë„ë³„ í†µê³„ (df_crime_yearly)\n",
    "districts = ['ê°•ë‚¨êµ¬', 'ê°•ë™êµ¬', 'ê°•ë¶êµ¬', 'ê°•ì„œêµ¬', 'ê´€ì•…êµ¬', \n",
    "             'ê´‘ì§„êµ¬', 'êµ¬ë¡œêµ¬', 'ê¸ˆì²œêµ¬', 'ë…¸ì›êµ¬', 'ë„ë´‰êµ¬',\n",
    "             'ë™ëŒ€ë¬¸êµ¬', 'ë™ì‘êµ¬', 'ë§ˆí¬êµ¬', 'ì„œëŒ€ë¬¸êµ¬', 'ì„œì´ˆêµ¬',\n",
    "             'ì„±ë™êµ¬', 'ì„±ë¶êµ¬', 'ì†¡íŒŒêµ¬', 'ì–‘ì²œêµ¬', 'ì˜ë“±í¬êµ¬',\n",
    "             'ìš©ì‚°êµ¬', 'ì€í‰êµ¬', 'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬', 'ì¤‘ë‘êµ¬']\n",
    "\n",
    "years = list(range(2014, 2024))\n",
    "crime_types = ['ì‚´ì¸', 'ê°•ë„', 'ì„±í­ë ¥', 'ì ˆë„', 'í­ë ¥']\n",
    "\n",
    "# ìì¹˜êµ¬ë³„ ë²”ì£„ ë°œìƒ íŠ¹ì„± (ê¸°ì¤€ê°’)\n",
    "district_crime_base = {\n",
    "    'ê°•ë‚¨êµ¬': {'ì‚´ì¸': 15, 'ê°•ë„': 45, 'ì„±í­ë ¥': 850, 'ì ˆë„': 4500, 'í­ë ¥': 6800},\n",
    "    'ê°•ë™êµ¬': {'ì‚´ì¸': 12, 'ê°•ë„': 35, 'ì„±í­ë ¥': 650, 'ì ˆë„': 3200, 'í­ë ¥': 5200},\n",
    "    'ê°•ë¶êµ¬': {'ì‚´ì¸': 18, 'ê°•ë„': 42, 'ì„±í­ë ¥': 720, 'ì ˆë„': 2800, 'í­ë ¥': 5800},\n",
    "    'ê°•ì„œêµ¬': {'ì‚´ì¸': 14, 'ê°•ë„': 38, 'ì„±í­ë ¥': 780, 'ì ˆë„': 3800, 'í­ë ¥': 6200},\n",
    "    'ê´€ì•…êµ¬': {'ì‚´ì¸': 16, 'ê°•ë„': 48, 'ì„±í­ë ¥': 920, 'ì ˆë„': 4200, 'í­ë ¥': 7200},\n",
    "    'ê´‘ì§„êµ¬': {'ì‚´ì¸': 11, 'ê°•ë„': 32, 'ì„±í­ë ¥': 680, 'ì ˆë„': 3400, 'í­ë ¥': 5400},\n",
    "    'êµ¬ë¡œêµ¬': {'ì‚´ì¸': 13, 'ê°•ë„': 40, 'ì„±í­ë ¥': 750, 'ì ˆë„': 3600, 'í­ë ¥': 5900},\n",
    "    'ê¸ˆì²œêµ¬': {'ì‚´ì¸': 10, 'ê°•ë„': 28, 'ì„±í­ë ¥': 580, 'ì ˆë„': 2600, 'í­ë ¥': 4200},\n",
    "    'ë…¸ì›êµ¬': {'ì‚´ì¸': 15, 'ê°•ë„': 36, 'ì„±í­ë ¥': 720, 'ì ˆë„': 3400, 'í­ë ¥': 5600},\n",
    "    'ë„ë´‰êµ¬': {'ì‚´ì¸': 9, 'ê°•ë„': 24, 'ì„±í­ë ¥': 480, 'ì ˆë„': 2200, 'í­ë ¥': 3800},\n",
    "    'ë™ëŒ€ë¬¸êµ¬': {'ì‚´ì¸': 14, 'ê°•ë„': 42, 'ì„±í­ë ¥': 780, 'ì ˆë„': 3800, 'í­ë ¥': 6000},\n",
    "    'ë™ì‘êµ¬': {'ì‚´ì¸': 11, 'ê°•ë„': 30, 'ì„±í­ë ¥': 620, 'ì ˆë„': 2800, 'í­ë ¥': 4600},\n",
    "    'ë§ˆí¬êµ¬': {'ì‚´ì¸': 13, 'ê°•ë„': 44, 'ì„±í­ë ¥': 820, 'ì ˆë„': 4000, 'í­ë ¥': 6400},\n",
    "    'ì„œëŒ€ë¬¸êµ¬': {'ì‚´ì¸': 10, 'ê°•ë„': 28, 'ì„±í­ë ¥': 560, 'ì ˆë„': 2600, 'í­ë ¥': 4400},\n",
    "    'ì„œì´ˆêµ¬': {'ì‚´ì¸': 12, 'ê°•ë„': 38, 'ì„±í­ë ¥': 720, 'ì ˆë„': 3800, 'í­ë ¥': 5600},\n",
    "    'ì„±ë™êµ¬': {'ì‚´ì¸': 10, 'ê°•ë„': 26, 'ì„±í­ë ¥': 540, 'ì ˆë„': 2600, 'í­ë ¥': 4200},\n",
    "    'ì„±ë¶êµ¬': {'ì‚´ì¸': 14, 'ê°•ë„': 36, 'ì„±í­ë ¥': 680, 'ì ˆë„': 3200, 'í­ë ¥': 5400},\n",
    "    'ì†¡íŒŒêµ¬': {'ì‚´ì¸': 16, 'ê°•ë„': 46, 'ì„±í­ë ¥': 880, 'ì ˆë„': 4400, 'í­ë ¥': 6800},\n",
    "    'ì–‘ì²œêµ¬': {'ì‚´ì¸': 11, 'ê°•ë„': 30, 'ì„±í­ë ¥': 600, 'ì ˆë„': 2800, 'í­ë ¥': 4600},\n",
    "    'ì˜ë“±í¬êµ¬': {'ì‚´ì¸': 18, 'ê°•ë„': 52, 'ì„±í­ë ¥': 980, 'ì ˆë„': 4800, 'í­ë ¥': 7600},\n",
    "    'ìš©ì‚°êµ¬': {'ì‚´ì¸': 12, 'ê°•ë„': 36, 'ì„±í­ë ¥': 680, 'ì ˆë„': 3400, 'í­ë ¥': 5200},\n",
    "    'ì€í‰êµ¬': {'ì‚´ì¸': 13, 'ê°•ë„': 32, 'ì„±í­ë ¥': 640, 'ì ˆë„': 3000, 'í­ë ¥': 5000},\n",
    "    'ì¢…ë¡œêµ¬': {'ì‚´ì¸': 14, 'ê°•ë„': 48, 'ì„±í­ë ¥': 860, 'ì ˆë„': 4200, 'í­ë ¥': 6200},\n",
    "    'ì¤‘êµ¬': {'ì‚´ì¸': 15, 'ê°•ë„': 50, 'ì„±í­ë ¥': 900, 'ì ˆë„': 4600, 'í­ë ¥': 6600},\n",
    "    'ì¤‘ë‘êµ¬': {'ì‚´ì¸': 16, 'ê°•ë„': 44, 'ì„±í­ë ¥': 760, 'ì ˆë„': 3400, 'í­ë ¥': 5800}\n",
    "}\n",
    "\n",
    "# ì—°ë„ë³„ íŠ¸ë Œë“œ (ê°ì†Œ ì¶”ì„¸)\n",
    "yearly_trend = {\n",
    "    2014: 1.15, 2015: 1.12, 2016: 1.08, 2017: 1.05, 2018: 1.02,\n",
    "    2019: 1.00, 2020: 0.92, 2021: 0.88, 2022: 0.90, 2023: 0.93\n",
    "}\n",
    "\n",
    "# ë°ì´í„° ìƒì„±\n",
    "yearly_records = []\n",
    "np.random.seed(42)\n",
    "\n",
    "for district in districts:\n",
    "    for year in years:\n",
    "        for crime in crime_types:\n",
    "            base = district_crime_base[district][crime]\n",
    "            trend = yearly_trend[year]\n",
    "            noise = np.random.normal(1, 0.1)\n",
    "            count = int(base * trend * noise)\n",
    "            yearly_records.append({\n",
    "                'ìì¹˜êµ¬': district,\n",
    "                'ì—°ë„': year,\n",
    "                'ë²”ì£„ìœ í˜•': crime,\n",
    "                'ë°œìƒê±´ìˆ˜': max(0, count)\n",
    "            })\n",
    "\n",
    "df_crime_yearly = pd.DataFrame(yearly_records)\n",
    "\n",
    "print(f\"âœ… df_time_final: {df_time_final.shape} - 5ëŒ€ ë²”ì£„ Ã— 8ê°œ ì‹œê°„ëŒ€\")\n",
    "print(f\"âœ… df_place_final: {df_place_final.shape} - 5ëŒ€ ë²”ì£„ Ã— 4ê°œ ì¥ì†Œ\")\n",
    "print(f\"âœ… df_crime_yearly: {df_crime_yearly.shape} - 25ê°œ ìì¹˜êµ¬ Ã— 10ë…„ Ã— 5ëŒ€ ë²”ì£„\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. í™•ë¥  ë¶„í¬ ê³„ì‚°\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š [Step 2] í™•ë¥  ë¶„í¬ ê³„ì‚°\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# 2-1. ì‹œê°„ëŒ€ë³„ ë°œìƒ ë¹„ìœ¨ ê³„ì‚°\n",
    "time_cols = [col for col in df_time_final.columns if col != 'ë²”ì£„ë¶„ë¥˜']\n",
    "df_time_prob = df_time_final.copy()\n",
    "\n",
    "for col in time_cols:\n",
    "    df_time_prob[f'{col}_ë¹„ìœ¨'] = df_time_prob[col] / df_time_prob[time_cols].sum(axis=1)\n",
    "\n",
    "time_prob_dict = {}\n",
    "for _, row in df_time_prob.iterrows():\n",
    "    crime = row['ë²”ì£„ë¶„ë¥˜']\n",
    "    time_prob_dict[crime] = {}\n",
    "    for col in time_cols:\n",
    "        time_prob_dict[crime][col] = row[f'{col}_ë¹„ìœ¨']\n",
    "\n",
    "print(\"âœ… ë²”ì£„ìœ í˜•ë³„ ì‹œê°„ëŒ€ ë°œìƒ í™•ë¥  ê³„ì‚° ì™„ë£Œ\")\n",
    "\n",
    "# 2-2. ì¥ì†Œë³„ ë°œìƒ ë¹„ìœ¨ ê³„ì‚°\n",
    "place_cols = [col for col in df_place_final.columns if col != 'ë²”ì£„ë¶„ë¥˜']\n",
    "df_place_prob = df_place_final.copy()\n",
    "\n",
    "for col in place_cols:\n",
    "    df_place_prob[f'{col}_ë¹„ìœ¨'] = df_place_prob[col] / df_place_prob[place_cols].sum(axis=1)\n",
    "\n",
    "place_prob_dict = {}\n",
    "for _, row in df_place_prob.iterrows():\n",
    "    crime = row['ë²”ì£„ë¶„ë¥˜']\n",
    "    place_prob_dict[crime] = {}\n",
    "    for col in place_cols:\n",
    "        place_prob_dict[crime][col] = row[f'{col}_ë¹„ìœ¨']\n",
    "\n",
    "print(\"âœ… ë²”ì£„ìœ í˜•ë³„ ì¥ì†Œ ë°œìƒ í™•ë¥  ê³„ì‚° ì™„ë£Œ\")\n",
    "\n",
    "# 2-3. ìì¹˜êµ¬ë³„ ë°œìƒ ë¹„ìœ¨ ê³„ì‚°\n",
    "district_crime_totals = df_crime_yearly.groupby(['ìì¹˜êµ¬', 'ë²”ì£„ìœ í˜•'])['ë°œìƒê±´ìˆ˜'].sum().reset_index()\n",
    "crime_totals = district_crime_totals.groupby('ë²”ì£„ìœ í˜•')['ë°œìƒê±´ìˆ˜'].sum()\n",
    "\n",
    "district_prob_dict = {}\n",
    "for crime in crime_types:\n",
    "    district_prob_dict[crime] = {}\n",
    "    crime_data = district_crime_totals[district_crime_totals['ë²”ì£„ìœ í˜•'] == crime]\n",
    "    total = crime_totals[crime]\n",
    "    for _, row in crime_data.iterrows():\n",
    "        district_prob_dict[crime][row['ìì¹˜êµ¬']] = row['ë°œìƒê±´ìˆ˜'] / total\n",
    "\n",
    "print(\"âœ… ë²”ì£„ìœ í˜•ë³„ ìì¹˜êµ¬ ë°œìƒ í™•ë¥  ê³„ì‚° ì™„ë£Œ\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì…‹ ìƒì„± (ëª¨ë“  ì¡°í•©)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š [Step 3] ìƒí™©ë³„ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì…‹ ìƒì„±\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ëª¨ë“  ì¡°í•© ìƒì„±\n",
    "combinations = list(product(districts, crime_types, place_cols, time_cols))\n",
    "print(f\"   ì´ ì¡°í•© ìˆ˜: {len(combinations):,}ê°œ\")\n",
    "print(f\"   (25 ìì¹˜êµ¬ Ã— 5 ë²”ì£„ìœ í˜• Ã— 4 ì¥ì†Œ Ã— 8 ì‹œê°„ëŒ€)\")\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "df_combinations = pd.DataFrame(combinations, \n",
    "                               columns=['ìì¹˜êµ¬', 'ë²”ì£„ìœ í˜•', 'ì¥ì†Œì¹´í…Œê³ ë¦¬', 'ì‹œê°„ëŒ€'])\n",
    "\n",
    "# ============================================================================\n",
    "# ê°œë³„ í™•ë¥  ê³„ì‚° ë° ì €ì¥ (ë¨¼ì €!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"   â³ ê°œë³„ í™•ë¥  ê³„ì‚° ì¤‘...\")\n",
    "\n",
    "# ê° í™•ë¥ ì„ ë³„ë„ ì»¬ëŸ¼ìœ¼ë¡œ ì €ì¥\n",
    "df_combinations['p_district'] = df_combinations.apply(\n",
    "    lambda x: district_prob_dict.get(x['ë²”ì£„ìœ í˜•'], {}).get(x['ìì¹˜êµ¬'], 0.04), axis=1)\n",
    "df_combinations['p_place'] = df_combinations.apply(\n",
    "    lambda x: place_prob_dict.get(x['ë²”ì£„ìœ í˜•'], {}).get(x['ì¥ì†Œì¹´í…Œê³ ë¦¬'], 0.25), axis=1)\n",
    "df_combinations['p_time'] = df_combinations.apply(\n",
    "    lambda x: time_prob_dict.get(x['ë²”ì£„ìœ í˜•'], {}).get(x['ì‹œê°„ëŒ€'], 0.125), axis=1)\n",
    "\n",
    "print(\"   âœ… ê°œë³„ í™•ë¥  ê³„ì‚° ì™„ë£Œ\")\n",
    "\n",
    "# ============================================================================\n",
    "# risk_score2 ê³„ì‚° ë° ìŠ¤ì¼€ì¼ë§\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def calculate_risk_score_v2(row):\n",
    "    \"\"\"ë§ì…ˆ ê¸°ë°˜ ìœ„í—˜ ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "    crime = row['ë²”ì£„ìœ í˜•']\n",
    "    \n",
    "    # ì´ë¯¸ ê³„ì‚°ëœ í™•ë¥  ì‚¬ìš©\n",
    "    p_district = row['p_district']\n",
    "    p_place = row['p_place']\n",
    "    p_time = row['p_time']\n",
    "    \n",
    "    crime_weight = {\n",
    "        'ì‚´ì¸': 5.0, 'ê°•ë„': 4.0, 'ì„±í­ë ¥': 3.5, 'í­ë ¥': 2.0, 'ì ˆë„': 1.5\n",
    "    }\n",
    "    \n",
    "    # ë§ì…ˆ ë°©ì‹ (ì›ë³¸)\n",
    "    risk_score = (p_district + p_place + p_time) * crime_weight.get(crime, 1.0) * 1000\n",
    "    \n",
    "    return risk_score\n",
    "\n",
    "# risk_score2 ê³„ì‚°\n",
    "print(\"\\n   â³ risk_score2 ê³„ì‚° ì¤‘...\")\n",
    "df_combinations['risk_score2'] = df_combinations.apply(calculate_risk_score_v2, axis=1)\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "df_combinations['risk_score'] = scaler.fit_transform(df_combinations[['risk_score2']])\n",
    "\n",
    "print(f\"\\nâœ… risk_score2 + Scaling ì™„ë£Œ!\")\n",
    "print(f\"   ì›ë³¸ ë²”ìœ„: [{df_combinations['risk_score2'].min():.2f}, {df_combinations['risk_score2'].max():.2f}]\")\n",
    "print(f\"   ìŠ¤ì¼€ì¼ í›„ (risk_score): [0.00, 100.00]\")\n",
    "\n",
    "print(f\"\\nâœ… ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {df_combinations.shape}\")\n",
    "print(f\"   ì»¬ëŸ¼: {list(df_combinations.columns)}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ìœ„í—˜ ë“±ê¸‰)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š [Step 4] íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ìƒìœ„ 30%ë¥¼ 'ìœ„í—˜'ìœ¼ë¡œ ë¶„ë¥˜\n",
    "threshold = df_combinations['risk_score'].quantile(0.70)\n",
    "df_combinations['is_high_risk'] = (df_combinations['risk_score'] >= threshold).astype(int)\n",
    "\n",
    "print(f\"   ìœ„í—˜ íŒë‹¨ ê¸°ì¤€ ì ìˆ˜: {threshold:.4f}\")\n",
    "print(f\"   ìœ„í—˜(1) ë¹„ìœ¨: {df_combinations['is_high_risk'].mean()*100:.1f}%\")\n",
    "print(f\"   ì•ˆì „(0) ë¹„ìœ¨: {(1-df_combinations['is_high_risk'].mean())*100:.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Feature Engineering - ì¸ì½”ë”©\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š [Step 5] Feature Engineering - ì¸ì½”ë”©\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df_ml = df_combinations.copy()\n",
    "\n",
    "# 5-1. Label Encoding\n",
    "label_encoders = {}\n",
    "\n",
    "categorical_cols = ['ìì¹˜êµ¬', 'ë²”ì£„ìœ í˜•', 'ì¥ì†Œì¹´í…Œê³ ë¦¬', 'ì‹œê°„ëŒ€']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_ml[f'{col}_encoded'] = le.fit_transform(df_ml[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"   âœ… {col}: {len(le.classes_)}ê°œ í´ë˜ìŠ¤ â†’ Label Encoding ì™„ë£Œ\")\n",
    "\n",
    "# 5-2. ì¶”ê°€ í”¼ì²˜ ìƒì„±\n",
    "# ì‹œê°„ëŒ€ë¥¼ ìˆ«ìë¡œ ë³€í™˜ (00ì‹œ-03ì‹œ â†’ 1.5, 03ì‹œ-06ì‹œ â†’ 4.5, ...)\n",
    "time_to_hour = {\n",
    "    '00ì‹œ-03ì‹œ': 1.5, '03ì‹œ-06ì‹œ': 4.5, '06ì‹œ-09ì‹œ': 7.5, '09ì‹œ-12ì‹œ': 10.5,\n",
    "    '12ì‹œ-15ì‹œ': 13.5, '15ì‹œ-18ì‹œ': 16.5, '18ì‹œ-21ì‹œ': 19.5, '21ì‹œ-24ì‹œ': 22.5\n",
    "}\n",
    "df_ml['ì‹œê°„_ì¤‘ì•™ê°’'] = df_ml['ì‹œê°„ëŒ€'].map(time_to_hour)\n",
    "\n",
    "# ì‹¬ì•¼ ì‹œê°„ëŒ€ ì—¬ë¶€ (21ì‹œ-06ì‹œ)\n",
    "df_ml['is_night'] = df_ml['ì‹œê°„ëŒ€'].isin(['21ì‹œ-24ì‹œ', '00ì‹œ-03ì‹œ', '03ì‹œ-06ì‹œ']).astype(int)\n",
    "\n",
    "# ì¶œí‡´ê·¼ ì‹œê°„ëŒ€ ì—¬ë¶€ (06ì‹œ-09ì‹œ, 18ì‹œ-21ì‹œ)\n",
    "df_ml['is_rush_hour'] = df_ml['ì‹œê°„ëŒ€'].isin(['06ì‹œ-09ì‹œ', '18ì‹œ-21ì‹œ']).astype(int)\n",
    "\n",
    "# ë²”ì£„ ì‹¬ê°ë„ ì ìˆ˜\n",
    "crime_severity = {'ì‚´ì¸': 5, 'ê°•ë„': 4, 'ì„±í­ë ¥': 4, 'í­ë ¥': 2, 'ì ˆë„': 1}\n",
    "df_ml['crime_severity'] = df_ml['ë²”ì£„ìœ í˜•'].map(crime_severity)\n",
    "\n",
    "print(\"\\n   âœ… ì¶”ê°€ íŒŒìƒ ë³€ìˆ˜ ìƒì„±:\")\n",
    "print(\"      - ì‹œê°„_ì¤‘ì•™ê°’: ì‹œê°„ëŒ€ì˜ ì¤‘ì•™ ì‹œê°\")\n",
    "print(\"      - is_night: ì‹¬ì•¼ ì‹œê°„ëŒ€ ì—¬ë¶€ (21ì‹œ-06ì‹œ)\")\n",
    "print(\"      - is_rush_hour: ì¶œí‡´ê·¼ ì‹œê°„ëŒ€ ì—¬ë¶€\")\n",
    "print(\"      - crime_severity: ë²”ì£„ ì‹¬ê°ë„ ì ìˆ˜\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. ìµœì¢… ML ë°ì´í„°ì…‹ êµ¬ì„±\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š [Step 6] ìµœì¢… ML ë°ì´í„°ì…‹ êµ¬ì„±\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# í”¼ì²˜ ì„ íƒ\n",
    "feature_cols = [\n",
    "    'ìì¹˜êµ¬_encoded', 'ë²”ì£„ìœ í˜•_encoded', 'ì¥ì†Œì¹´í…Œê³ ë¦¬_encoded', 'ì‹œê°„ëŒ€_encoded',\n",
    "    'p_district', 'p_place', 'p_time',\n",
    "    'ì‹œê°„_ì¤‘ì•™ê°’', 'is_night', 'is_rush_hour', 'crime_severity'\n",
    "]\n",
    "\n",
    "X = df_ml[feature_cols]\n",
    "y = df_ml['is_high_risk']\n",
    "\n",
    "# ìµœì¢… ML ë°ì´í„°í”„ë ˆì„\n",
    "df_final_ml = df_ml[['ìì¹˜êµ¬', 'ë²”ì£„ìœ í˜•', 'ì¥ì†Œì¹´í…Œê³ ë¦¬', 'ì‹œê°„ëŒ€'] + \n",
    "                    feature_cols + ['risk_score', 'is_high_risk']]\n",
    "\n",
    "print(f\"âœ… df_final_ml ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"   - í˜•íƒœ: {df_final_ml.shape}\")\n",
    "print(f\"   - í”¼ì²˜ ìˆ˜: {len(feature_cols)}ê°œ\")\n",
    "print(f\"   - íƒ€ê²Ÿ: is_high_risk (ì´ì§„ ë¶„ë¥˜)\")\n",
    "\n",
    "# ë°ì´í„° êµ¬ì¡° ì¶œë ¥\n",
    "print(\"\\nğŸ“‹ df_final_ml êµ¬ì¡°:\")\n",
    "print(\"-\" * 80)\n",
    "print(df_final_ml.head(10).to_string())\n",
    "\n",
    "print(\"\\nğŸ“‹ í”¼ì²˜ í†µê³„:\")\n",
    "print(\"-\" * 80)\n",
    "print(df_final_ml[feature_cols].describe().round(4).to_string())\n",
    "\n",
    "# ============================================================================\n",
    "# 7. ëª¨ë¸ í•™ìŠµ - RandomForest\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š [Step 7] RandomForest ëª¨ë¸ í•™ìŠµ\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  (8:2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"   í•™ìŠµ ë°ì´í„°: {X_train.shape[0]:,}ê°œ\")\n",
    "print(f\"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]:,}ê°œ\")\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\n   â³ ëª¨ë¸ í•™ìŠµ ì¤‘...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"   âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# ============================================================================\n",
    "# 8. ëª¨ë¸ í‰ê°€\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š [Step 8] ëª¨ë¸ í‰ê°€\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n   ğŸ¯ ì •í™•ë„ (Accuracy): {accuracy*100:.2f}%\")\n",
    "\n",
    "print(\"\\nğŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "print(\"-\" * 80)\n",
    "print(classification_report(y_test, y_pred, target_names=['ì•ˆì „(0)', 'ìœ„í—˜(1)']))\n",
    "\n",
    "print(\"\\nğŸ“‹ í˜¼ë™ í–‰ë ¬ (Confusion Matrix):\")\n",
    "print(\"-\" * 80)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"                ì˜ˆì¸¡: ì•ˆì „    ì˜ˆì¸¡: ìœ„í—˜\")\n",
    "print(f\"   ì‹¤ì œ: ì•ˆì „      {cm[0,0]:>6,}      {cm[0,1]:>6,}\")\n",
    "print(f\"   ì‹¤ì œ: ìœ„í—˜      {cm[1,0]:>6,}      {cm[1,1]:>6,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. Feature Importance ë¶„ì„ ë° ì‹œê°í™”\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“Š [Step 9] Feature Importance ë¶„ì„\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Feature Importance ì¶”ì¶œ\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nğŸ“‹ í”¼ì²˜ ì¤‘ìš”ë„ ìˆœìœ„:\")\n",
    "print(\"-\" * 80)\n",
    "for i, row in feature_importance.iterrows():\n",
    "    bar = \"â–ˆ\" * int(row['importance'] * 50)\n",
    "    print(f\"   {row['feature']:25s}: {row['importance']:.4f} {bar}\")\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# 9-1. Feature Importance ë°” ì°¨íŠ¸\n",
    "ax1 = axes[0, 0]\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(feature_importance)))\n",
    "bars = ax1.barh(feature_importance['feature'], feature_importance['importance'], color=colors)\n",
    "ax1.set_xlabel('Importance Score', fontsize=12)\n",
    "ax1.set_title('Feature Importance (RandomForest)', fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# ê°’ í‘œì‹œ\n",
    "for bar, val in zip(bars, feature_importance['importance']):\n",
    "    ax1.text(val + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "             f'{val:.3f}', va='center', fontsize=10)\n",
    "\n",
    "# 9-2. ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ìš”ë„ ê·¸ë£¹í™”\n",
    "ax2 = axes[0, 1]\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ ê·¸ë£¹í•‘\n",
    "category_groups = {\n",
    "    'Location': ['ìì¹˜êµ¬_encoded', 'p_district'],\n",
    "    'Crime Type': ['ë²”ì£„ìœ í˜•_encoded', 'crime_severity'],\n",
    "    'Place': ['ì¥ì†Œì¹´í…Œê³ ë¦¬_encoded', 'p_place'],\n",
    "    'Time': ['ì‹œê°„ëŒ€_encoded', 'p_time', 'ì‹œê°„_ì¤‘ì•™ê°’', 'is_night', 'is_rush_hour']\n",
    "}\n",
    "\n",
    "group_importance = {}\n",
    "for group, features in category_groups.items():\n",
    "    total = feature_importance[feature_importance['feature'].isin(features)]['importance'].sum()\n",
    "    group_importance[group] = total\n",
    "\n",
    "group_df = pd.DataFrame(list(group_importance.items()), columns=['Category', 'Importance'])\n",
    "group_df = group_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "colors2 = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "bars2 = ax2.barh(group_df['Category'], group_df['Importance'], color=colors2)\n",
    "ax2.set_xlabel('Combined Importance Score', fontsize=12)\n",
    "ax2.set_title('Feature Category Importance', fontsize=14, fontweight='bold')\n",
    "\n",
    "for bar, val in zip(bars2, group_df['Importance']):\n",
    "    ax2.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{val:.3f}', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 9-3. ìœ„í—˜ ì ìˆ˜ ë¶„í¬\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(df_ml[df_ml['is_high_risk']==0]['risk_score'], bins=50, alpha=0.7, \n",
    "         label='Safe (0)', color='#2ecc71', edgecolor='white')\n",
    "ax3.hist(df_ml[df_ml['is_high_risk']==1]['risk_score'], bins=50, alpha=0.7, \n",
    "         label='High Risk (1)', color='#e74c3c', edgecolor='white')\n",
    "ax3.axvline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold ({threshold:.3f})')\n",
    "ax3.set_xlabel('Risk Score', fontsize=12)\n",
    "ax3.set_ylabel('Frequency', fontsize=12)\n",
    "ax3.set_title('Risk Score Distribution by Class', fontsize=14, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "\n",
    "# 9-4. ì‹œê°„ëŒ€ë³„ ìœ„í—˜ ë¹„ìœ¨\n",
    "ax4 = axes[1, 1]\n",
    "time_risk = df_ml.groupby('ì‹œê°„ëŒ€')['is_high_risk'].mean().sort_index()\n",
    "time_order = ['00ì‹œ-03ì‹œ', '03ì‹œ-06ì‹œ', '06ì‹œ-09ì‹œ', '09ì‹œ-12ì‹œ', \n",
    "              '12ì‹œ-15ì‹œ', '15ì‹œ-18ì‹œ', '18ì‹œ-21ì‹œ', '21ì‹œ-24ì‹œ']\n",
    "time_risk = time_risk.reindex(time_order)\n",
    "\n",
    "colors3 = ['#e74c3c' if v > 0.3 else '#f39c12' if v > 0.25 else '#2ecc71' for v in time_risk.values]\n",
    "bars3 = ax4.bar(range(len(time_risk)), time_risk.values, color=colors3, edgecolor='white', linewidth=2)\n",
    "ax4.set_xticks(range(len(time_risk)))\n",
    "ax4.set_xticklabels([t.replace('ì‹œ-', '-').replace('ì‹œ', 'h') for t in time_order], rotation=45, ha='right')\n",
    "ax4.set_ylabel('High Risk Ratio', fontsize=12)\n",
    "ax4.set_title('High Risk Ratio by Time Period', fontsize=14, fontweight='bold')\n",
    "ax4.axhline(0.3, color='red', linestyle='--', alpha=0.7, label='30% threshold')\n",
    "ax4.legend()\n",
    "\n",
    "# ê°’ í‘œì‹œ\n",
    "for bar, val in zip(bars3, time_risk.values):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{val:.1%}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../final_output/final_viz/feature_importance_analysis.png', dpi=150, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(\"\\nâœ… ì‹œê°í™” ì €ì¥: feature_importance_analysis.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# 10. ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ‰ ìµœì¢… ê²°ê³¼ ìš”ì•½\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ“Œ ë°ì´í„°ì…‹ ì •ë³´\n",
    "   - ì´ ìƒ˜í”Œ ìˆ˜: {len(df_final_ml):,}ê°œ\n",
    "   - í”¼ì²˜ ìˆ˜: {len(feature_cols)}ê°œ\n",
    "   - íƒ€ê²Ÿ: is_high_risk (ìœ„í—˜: 30%, ì•ˆì „: 70%)\n",
    "\n",
    "ğŸ“Œ ëª¨ë¸ ì„±ëŠ¥\n",
    "   - ì •í™•ë„: {accuracy*100:.2f}%\n",
    "   - í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¹„ìœ¨: 80:20\n",
    "\n",
    "ğŸ“Œ ì£¼ìš” ì¸ì‚¬ì´íŠ¸ (Feature Importance)\n",
    "   1. ê°€ì¥ ì¤‘ìš”í•œ í”¼ì²˜: {feature_importance.iloc[0]['feature']} ({feature_importance.iloc[0]['importance']:.4f})\n",
    "   2. ë‘ ë²ˆì§¸ ì¤‘ìš”: {feature_importance.iloc[1]['feature']} ({feature_importance.iloc[1]['importance']:.4f})\n",
    "   3. ì„¸ ë²ˆì§¸ ì¤‘ìš”: {feature_importance.iloc[2]['feature']} ({feature_importance.iloc[2]['importance']:.4f})\n",
    "\n",
    "ğŸ“Œ ì¹´í…Œê³ ë¦¬ë³„ ì˜í–¥ë ¥\n",
    "\"\"\")\n",
    "\n",
    "for cat, imp in sorted(group_importance.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"   - {cat}: {imp:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# CSV ì €ì¥\n",
    "df_final_ml.to_csv('../final_output/df_final_ml_min_max.csv', index=False, encoding='cp949')\n",
    "print(\"ğŸ’¾ í•™ìŠµìš© ë°ì´í„°ì…‹ ì €ì¥: df_final_ml.csv\")\n",
    "\n",
    "# Feature Importance ì €ì¥\n",
    "feature_importance.to_csv('../final_output/feature_importance_min_max.csv', index=False)\n",
    "print(\"ğŸ’¾ í”¼ì²˜ ì¤‘ìš”ë„ ì €ì¥: feature_importance.csv\")\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥ (ì„ íƒ)\n",
    "import joblib\n",
    "joblib.dump(rf_model, '../final_output/rf_crime_model_min_max.pkl')\n",
    "joblib.dump(label_encoders, '../final_output/label_encoders_min_max.pkl')\n",
    "print(\"ğŸ’¾ ëª¨ë¸ ì €ì¥: rf_crime_model_min_max.pkl, label_encoders_min_max.pkl\")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "luckyseven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
